package org.allenai.pipeline.spark

import org.allenai.pipeline._
import org.allenai.pipeline.s3.CreateCoreArtifacts

import com.amazonaws.auth.BasicAWSCredentials
import com.typesafe.config.Config
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD

import scala.reflect.ClassTag

import java.net.URI

/** Created by rodneykinney on 5/24/15.
  */
trait SparkPipeline {
  this: Pipeline =>

  def sparkContext: SparkContext

  def persistRdd[T: ClassTag: StringSerializable](
    original: Producer[RDD[T]],
    suffix: String = ""
  ) = {
    val io = new PartitionedRddIo[T](sparkContext)
    val path = s"data/${autoGeneratedPath(original, io)}$suffix"
    val url = absoluteOutputUrl(path)
    persist[RDD[T], PartitionedRddArtifact[FlatArtifact]](original, io, url)
  }
}

trait ConfiguredSparkPipeline extends SparkPipeline {
  this: ConfiguredPipeline =>

  def optionallyPersistRdd[T: ClassTag: StringSerializable](
    original: Producer[RDD[T]],
    stepName: String,
    suffix: String = ""
  ) = {
    val io = new PartitionedRddIo[T](sparkContext)
    optionallyPersist(original, stepName, io, suffix)
  }
}

object SparkPipeline {
  // Create a Pipeline that writes output to the given root path in S3
  def apply(sc: SparkContext, credentials: BasicAWSCredentials, bucket: String, rootPath: String) = {
    val artifactFactory =
      ArtifactFactory(
        CreateCoreArtifacts.fromFileOrS3Urls(credentials),
        CreateRddArtifacts.fromFileOrS3Urls(credentials)
      )
    val rootUrl = new URI("s3", bucket, rootPath, null)
    new Pipeline(rootUrl, artifactFactory) with SparkPipeline {
      val sparkContext = sc
    }
  }

  def configured(sc: SparkContext, config: Config, credentials: BasicAWSCredentials) = {
    val artifactFactory = ArtifactFactory(
      CreateCoreArtifacts.fromFileOrS3Urls(credentials),
      CreateRddArtifacts.fromFileOrS3Urls(credentials)
    )
    new ConfiguredPipeline(config, artifactFactory) with ConfiguredSparkPipeline {
      val sparkContext = sc
    }
  }
}
